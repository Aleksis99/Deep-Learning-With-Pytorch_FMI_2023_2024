{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktTpk9bCr_Lg"
      },
      "source": [
        "# **PyTorch Basics Tutorial**\n",
        "\n",
        "The following tutorial is adapted from the [DeepLearningForNLPInPytorch tutorial](https://github.com/rguthrie3/DeepLearningForNLPInPytorch/blob/master/Deep%20Learning%20for%20Natural%20Language%20Processing%20with%20Pytorch.ipynb) and the [Official PyTorch tutorials](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-s9_PdslEW"
      },
      "source": [
        "## Installation\n",
        "\n",
        "**Via Anaconda/Miniconda:**  \n",
        "`condainstall pytorch-c pytorch`\n",
        "\n",
        "**Via pip:**  \n",
        "`pip3 install torch`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GlRjjJqDr5bs",
        "outputId": "bee013fc-fbf9-443e-ed38-028ce004b99b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaso_dtjsExr"
      },
      "source": [
        "## 1. Tensors\n",
        "Tensors are similar to NumPyâ€™s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
        "\n",
        "### 1.1 Creating Tensors from data\n",
        "Tensors can be created from Python lists with the `torch.Tensor()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "zpQ5Oek2s4ca",
        "outputId": "c144e664-ea55-49e5-9b8d-eeb47f8f38ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example with 1-D data\n",
            "tensor([1., 2., 3.])\n",
            "\n",
            "Example with 2-D data\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "\n",
            "Example with 3-D data\n",
            "tensor([[[1., 2.],\n",
            "         [3., 4.]],\n",
            "\n",
            "        [[5., 6.],\n",
            "         [7., 8.]]])\n"
          ]
        }
      ],
      "source": [
        "# Example with 1-D data\n",
        "data = [1.0, 2.0, 3.0]\n",
        "tensor = torch.Tensor(data)\n",
        "print(\"Example with 1-D data\")\n",
        "print(tensor)\n",
        "\n",
        "# Example with 2-D data\n",
        "data = [[1., 2., 3.], [4., 5., 6]]\n",
        "tensor = torch.Tensor(data)\n",
        "print(\"\\nExample with 2-D data\")\n",
        "print(tensor)\n",
        "\n",
        "# Example with 3-D data\n",
        "data = [[[1.,2.], [3.,4.]],\n",
        "        [[5.,6.], [7.,8.]]]\n",
        "tensor = torch.Tensor(data)\n",
        "print(\"\\nExample with 3-D data\")\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b18oELGNvhNg"
      },
      "source": [
        "### 1.2 Initializing an empty Tensor\n",
        "An uninitialized matrix is declared, but does not contain definite known values before it is used. When an uninitialized matrix is created, whatever values were in the allocated memory at the time will appear as the initial values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "kg_gi1AWv_1E",
        "outputId": "419ff614-1489-4756-e6d8-2c04dbe600f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-4.6936e-10,  1.3523e-42,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
          ]
        }
      ],
      "source": [
        "# Construct a 2x3 matrix, uninitialized\n",
        "x = torch.empty(2, 3)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDr4ZzQrwOmZ"
      },
      "source": [
        "### 1.3 Randomly initialized Tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "r0pZQfUywgwL",
        "outputId": "07127830-6fab-4148-8c0b-f8f7921ee912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5676, 0.0040, 0.9728],\n",
            "        [0.2527, 0.6548, 0.8580]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(2, 3)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLJOlS32wjFg"
      },
      "source": [
        "### 1.4 Tensor with zeros or ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "JQ0JM9njwtFr",
        "outputId": "904621ac-cc00-42eb-adfe-3f2dd9c5b44b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrix of zeros\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "\n",
            "Matrix of zeros typecasted to long\n",
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0]])\n",
            "\n",
            "Matrix of ones typecasted to long\n",
            "tensor([[1, 1, 1],\n",
            "        [1, 1, 1]])\n"
          ]
        }
      ],
      "source": [
        "# Create a matrix of all zeros\n",
        "x = torch.zeros(2, 3)\n",
        "print(\"Matrix of zeros\")\n",
        "print(x)\n",
        "\n",
        "# Create a matrix of all zeros and explicitly set data type to be long int\n",
        "x = torch.zeros(2, 3, dtype=torch.long)\n",
        "print(\"\\nMatrix of zeros typecasted to long\")\n",
        "print(x)\n",
        "\n",
        "x = torch.ones(2, 3, dtype=torch.long)\n",
        "print(\"\\nMatrix of ones typecasted to long\")\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0HQKRy2wsDd"
      },
      "source": [
        "### 1.5 Create Tensor based on existing Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "visdOehpxqd-",
        "outputId": "58b499ef-6347-4e68-a00c-a6b42df513eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float64)\n",
            "tensor([[-0.1112,  0.3351,  1.0566],\n",
            "        [ 1.6894, -0.7366,  0.6030]])\n"
          ]
        }
      ],
      "source": [
        "x = x.new_ones(2, 3, dtype=torch.double)      # new_* methods take in sizes\n",
        "print(x)\n",
        "\n",
        "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
        "print(x)                                      # result has the same size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLZfCcObzBTe"
      },
      "source": [
        "### 1.6 Size of a Tensor\n",
        "`torch.Size` is in fact a tuple, so it supports all tuple operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "YVIu-uZnzEm-",
        "outputId": "0a2d1cf3-a6f2-4fa8-f1c9-bb3108b8fbf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example with 1-D data\n",
            "tensor([1., 2., 3.])\n",
            "torch.Size([3])\n",
            "\n",
            "Example with 2-D data\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "torch.Size([2, 3])\n",
            "\n",
            "Example with 3-D data\n",
            "tensor([[[1., 2.],\n",
            "         [3., 4.]],\n",
            "\n",
            "        [[5., 6.],\n",
            "         [7., 8.]]])\n",
            "torch.Size([2, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "# Example with 1-D data\n",
        "data = [1.0, 2.0, 3.0]\n",
        "tensor = torch.Tensor(data)\n",
        "print(\"Example with 1-D data\")\n",
        "print(tensor)\n",
        "print(tensor.size())\n",
        "\n",
        "# Example with 2-D data\n",
        "data = [[1., 2., 3.], [4., 5., 6]]\n",
        "tensor = torch.Tensor(data)\n",
        "print(\"\\nExample with 2-D data\")\n",
        "print(tensor)\n",
        "print(tensor.size())\n",
        "\n",
        "# Example with 3-D data\n",
        "data = [[[1.,2.], [3.,4.]],\n",
        "        [[5.,6.], [7.,8.]]]\n",
        "tensor = torch.Tensor(data)\n",
        "print(\"\\nExample with 3-D data\")\n",
        "print(tensor)\n",
        "print(tensor.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYai6C6H0kKx"
      },
      "source": [
        "### 1.7 Operations with Tensors\n",
        "Most operations are very similar to NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "ZRj42hPO0prW",
        "outputId": "d9296ddb-1488-4ae7-ba44-eedf2bd93710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([5., 7., 9.])\n",
            "tensor([5., 7., 9.])\n",
            "tensor([5., 7., 9.])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_15800\\1344850032.py:14: UserWarning: An output with one or more elements was resized since it had shape [2, 3], which does not match the required output shape [3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\Resize.cpp:35.)\n",
            "  torch.add(x, y, out=output)\n"
          ]
        }
      ],
      "source": [
        "# Addition\n",
        "x = torch.Tensor([ 1., 2., 3. ])\n",
        "y = torch.Tensor([ 4., 5., 6. ])\n",
        "\n",
        "# using arithmetic operation\n",
        "z = x + y\n",
        "print(z)\n",
        "\n",
        "# using method\n",
        "print(torch.add(x, y))\n",
        "\n",
        "# using method and providing an output tensor as argument\n",
        "output = torch.empty(2, 3)\n",
        "torch.add(x, y, out=output)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cjy0Zqik0-wL",
        "outputId": "d2820c51-6af4-421b-dbc6-e359e4ace0e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([5., 7., 9.])\n"
          ]
        }
      ],
      "source": [
        "# In-place addition\n",
        "\n",
        "x = torch.Tensor([ 1., 2., 3. ])\n",
        "y = torch.Tensor([ 4., 5., 6. ])\n",
        "\n",
        "y.add_(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFetPkbO1750"
      },
      "source": [
        "Any operation that mutates a tensor in-place is post-fixed with an underscore `_`. For example: `x.copy_(y)`, `x.t_()`, will change `x`.\n",
        "\n",
        "See [the PyTorch official documentation](http://pytorch.org/docs/torch.html) for a complete list of the massive number of operations available to you.  They expand beyond just mathematical operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gedfbDbd06Fn",
        "outputId": "a513960b-36e4-4eae-f930-71acd291059e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2., 5.])\n"
          ]
        }
      ],
      "source": [
        "# Indexing\n",
        "\n",
        "x = torch.Tensor([[1., 2., 3.], \n",
        "                  [4., 5., 6]])\n",
        "print(x[:, 1]) # Gets column with index 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IqeYxdg3Bvk"
      },
      "source": [
        "### 1.8 Reshaping Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pUwsXsfw26g_",
        "outputId": "b2745122-8691-4c16-e51a-b13296dbcc45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
        "print(x.size(), y.size(), z.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "clJzuzI93U7k",
        "outputId": "0cb14624-c1af-4694-9a8f-9bd2ca7c9f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.1956])\n",
            "1.1955841779708862\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(1)\n",
        "print(x)\n",
        "print(x.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRbkdZcArlk7"
      },
      "source": [
        "### 1.9 Converting to and from NumPy\n",
        "Converting a Torch Tensor to a NumPy array and vice versa is a breeze.\n",
        "\n",
        "The Torch Tensor and NumPy array will **share their underlying memory locations** (if the Torch Tensor is on CPU), and **changing one will change the other**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "BlLtucZM3g-N",
        "outputId": "cf9dda0e-a2f9-4182-e50d-254129dc28c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original a: tensor([1., 1., 1., 1., 1.])\n",
            "Original b: [1. 1. 1. 1. 1.]\n",
            "New a: tensor([2., 2., 2., 2., 2.])\n",
            "New b: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ],
      "source": [
        "a = torch.ones(5)\n",
        "print(\"Original a:\", a)\n",
        "\n",
        "b = a.numpy()\n",
        "print(\"Original b:\", b)\n",
        "\n",
        "a.add_(1)\n",
        "print(\"New a:\", a)\n",
        "print(\"New b:\", b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Cij37AWq31XG",
        "outputId": "19469cfa-83f2-4e65-a1f9-a9d54aacfde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "a = np.ones(5)\n",
        "b = torch.from_numpy(a)\n",
        "np.add(a, 1, out=a)\n",
        "print(a)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-T79NfE4Ck2"
      },
      "source": [
        "### 1.10. CUDA Tensors\n",
        "\n",
        "Tensors can be moved onto any device using the `.to` method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbUIutU44psf"
      },
      "source": [
        "### Enable GPU on your Colab notebook\n",
        "\n",
        "Go to Edit -> Notebook Settings -> select GPU as Hardware accelerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "KWaJxd1-4M2w",
        "outputId": "b6ee3c7f-7039-404c-d762-16f9fbe725c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available? True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2., 3., 4.], device='cuda:0')\n",
            "tensor([2., 3., 4.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# Try to run this cell with both GPU support and without\n",
        "import torch\n",
        "print(\"CUDA available?\", torch.cuda.is_available())\n",
        "\n",
        "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")          # a CUDA device object\n",
        "    x = torch.Tensor([1.0, 2.0, 3.0])\n",
        "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
        "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
        "    z = x + y\n",
        "    print(z)\n",
        "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqClHzpw6Lg3"
      },
      "source": [
        "## 2. Autograd: Automatic Differentiation\n",
        "\n",
        "The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
        "\n",
        "``torch.Tensor`` is the central class of the package. If you set its attribute\n",
        "``.requires_grad`` as ``True``, it **starts to track all operations on it**. When\n",
        "you finish your computation you can call ``.backward()`` and have **all the\n",
        "gradients computed automatically**. The gradient for this tensor will be\n",
        "accumulated into ``.grad`` attribute.\n",
        "\n",
        "To **stop a tensor from tracking history**, you can call ``.detach()`` to detach\n",
        "it from the computation history, and to prevent future computation from being\n",
        "tracked.\n",
        "\n",
        "To **prevent tracking history (and using memory)**, you can also wrap the code block\n",
        "in ``with torch.no_grad():``. This can be particularly helpful when evaluating a\n",
        "model because the model may have trainable parameters with `requires_grad=True`,\n",
        "but for which we don't need the gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "s3JXX8xr8HWJ",
        "outputId": "e7ed5c89-4fe8-451d-ed93-ef0f7f1b1600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "CuHGuSWn8Pfm",
        "outputId": "0007a45b-0a94-4029-889c-c3cfc6db9fe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "y = x + 2\n",
        "print(y)\n",
        "\n",
        "# y was created as a result of an operation, so it has a grad_fn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ylMfa4AP8ZfW",
        "outputId": "b786fa1d-d5d8-4847-ff70-a7de3ebab1f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(z, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbbCf8kM8-hd"
      },
      "source": [
        "``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
        "flag in-place. The input flag defaults to ``False`` if not given.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "qrCqBdbE9Avl",
        "outputId": "5893ce98-b644-43c3-9869-a377ab6b8cd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "<SumBackward0 object at 0x00000152CC12BDC0>\n"
          ]
        }
      ],
      "source": [
        "a = torch.randn(2, 2)\n",
        "a = ((a * 3) / (a - 1))\n",
        "print(a.requires_grad)\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)\n",
        "b = (a * a).sum()\n",
        "print(b.grad_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gonQpAmB-nLL"
      },
      "source": [
        "Let us consider a complete example now.\n",
        "\n",
        "Let $out = \\frac{1}{4}\\sum_i z_i$,  \n",
        "\n",
        "$z_i = 3(x_i+2)^2$  \n",
        "\n",
        "and $z_i\\bigr\\rvert_{x_i=1} = 27$.  \n",
        "\n",
        "Therefore,  \n",
        "\n",
        "$\\frac{\\partial out}{\\partial x_i} = \\frac{1}{4}\\frac{\\partial z_i}{\\partial x_i} = \\frac{1}{4}.3.2(x_i+2) = \\frac{3}{2}(x_i+2)$,  \n",
        "\n",
        " hence  \n",
        "\n",
        "$\\frac{\\partial out}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "TbbdshuP-mRz",
        "outputId": "4fc772f9-4f20-44d7-b59d-f7b6da1a5590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(27., grad_fn=<MeanBackward0>)\n",
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "out.backward()\n",
        "print(out)\n",
        "print(x.grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a tensor(-4.)\n",
            "b tensor(-4.)\n",
            "c tensor(6.)\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor(4., requires_grad=True)\n",
        "b = torch.tensor(2., requires_grad=True)\n",
        "c = torch.tensor(-4., requires_grad=True)\n",
        "\n",
        "d = a + b\n",
        "e = d * c\n",
        "e.backward()\n",
        "\n",
        "print('a', a.grad)\n",
        "print('b', b.grad)\n",
        "print('c', c.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZylx24TAy1q"
      },
      "source": [
        "Mathematically, if you have a vector valued function $\\vec{y}=f(\\vec{x})$, then the gradient of $\\vec{y}$ with respect to $\\vec{x}$ is a Jacobian matrix:\n",
        "\\begin{split}J=\\left(\\begin{array}{ccc}\n",
        " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        " \\vdots & \\ddots & \\vdots\\\\\n",
        " \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        " \\end{array}\\right)\\end{split}\n",
        "\n",
        "Generally speaking, `torch.autograd` is an engine for computing vector-Jacobian product. That is, given any vector $v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$, compute the product $v^{T}\\cdot J$. If $v$ happens to be the gradient of a scalar function $l=g\\left(\\vec{y}\\right)$, that is, $v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$, then by the chain rule, the vector-Jacobian product would be the gradient of $l$ with respect to $\\vec{x}$:\n",
        "\\begin{split}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
        " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
        " \\vdots & \\ddots & \\vdots\\\\\n",
        " \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        " \\end{array}\\right)\\left(\\begin{array}{c}\n",
        " \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
        " \\vdots\\\\\n",
        " \\frac{\\partial l}{\\partial y_{m}}\n",
        " \\end{array}\\right)=\\left(\\begin{array}{c}\n",
        " \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
        " \\vdots\\\\\n",
        " \\frac{\\partial l}{\\partial x_{n}}\n",
        " \\end{array}\\right)\\end{split}\n",
        "\n",
        "(Note that $v^{T}\\cdot J$ gives a row vector which can be treated as a column vector by taking $J^{T}\\cdot v$.)\n",
        "\n",
        "This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk1g2FU8YwfS"
      },
      "source": [
        "You can also stop autograd from tracking history on Tensors with `.requires_grad=True` either by wrapping the code block in with `torch.no_grad()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "h-tRLTbhYtWL",
        "outputId": "a4235b99-e7da-4bb1-e8c5-cb326d15cb46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    print((x ** 2).requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5716VvqZjfV"
      },
      "source": [
        "Every time a variable is back propogated through, the gradient will be accumulated instead of being replaced. Calling `tensor.grad_zero()` would reset the gradients that have accumulated to 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-20.4000, -20.4000, -20.4000, -20.4000],\n",
            "        [-24.0000, -24.0000, -24.0000, -24.0000],\n",
            "        [-28.2000, -28.2000, -28.2000, -28.2000]])\n",
            "tensor([[-2.2342, -5.2513, -5.0818],\n",
            "        [-2.2342, -5.2513, -5.0818]])\n"
          ]
        }
      ],
      "source": [
        "X = torch.tensor([[1.2,2,3],[4,5,6]])\n",
        "Y = torch.tensor([[3,2,1],[2,3,4.1]], requires_grad=True)\n",
        "A = torch.rand(3,4, requires_grad=True)\n",
        "B = torch.matmul(X+Y,A)\n",
        "C = torch.sum(-2 * B)\n",
        "C.backward()\n",
        "\n",
        "print(A.grad)\n",
        "print(Y.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-20.3167, -20.3167, -20.3167, -20.3167],\n",
            "        [-23.9167, -23.9167, -23.9167, -23.9167],\n",
            "        [-28.1167, -28.1167, -28.1167, -28.1167]])\n",
            "tensor([[-2.2342, -5.2513, -5.0818],\n",
            "        [-2.2342, -5.2513, -5.0818]])\n"
          ]
        }
      ],
      "source": [
        "S = torch.mean(A)\n",
        "S.backward()\n",
        "print(A.grad)\n",
        "print(Y.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-1.5765)\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    print(torch.mean(torch.matmul(Y,A)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5831, 0.5831, 0.5831, 0.5831],\n",
            "        [0.5265, 0.5265, 0.5265, 0.5265],\n",
            "        [0.5422, 0.5422, 0.5422, 0.5422]])\n",
            "tensor([[-0.2354, -0.0468, -0.0649],\n",
            "        [-0.2354, -0.0468, -0.0649]])\n",
            "tensor([[3.1149, 1.6623, 0.6967],\n",
            "        [2.1149, 2.6623, 3.7967]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "A.grad = None\n",
        "Y.grad = None\n",
        "\n",
        "S = torch.mean(torch.matmul(Y,A))\n",
        "S.backward()\n",
        "\n",
        "print(A.grad)\n",
        "print(Y.grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    A -= 1.2 * A.grad\n",
        "    Y -= 1.2 * Y.grad\n",
        "\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-6.3826)\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    print(torch.mean(torch.matmul(Y,A)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
